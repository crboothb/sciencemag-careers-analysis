{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "imported\n"
    }
   ],
   "source": [
    "# import os\n",
    "# from bs4 import BeautifulSoup, NavigableString, Tag \n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "# from sets import Set\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "imported\n"
    }
   ],
   "source": [
    "# IMPORTS SPECIFIC TO TOPIC MODELING\n",
    "\n",
    "# import langid\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import editorial text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_func as imp\n",
    "import tags_work as tgs\n",
    "import desc_vis as vis\n",
    "import classifier_func as cls\n",
    "import classifier_help as clh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents and Splitting Into Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     id                                          headline  \\\n0  5839                     how to write a winning résumé   \n1  5835         the commandments of cover letter creation   \n2  5840  dressing scientists for success: male case study   \n\n                                                tags      authors       date  \\\n0  [tooling up, advice, graduate, academic, indus...  peter fiske 1996-10-18   \n1                     [tooling up, advice, americas]  peter fiske 1996-12-20   \n2  [tooling up, advice, graduate, postdoc, academ...  peter fiske 1997-09-26   \n\n      time                                               text  \\\n0  8:00 am    by \\nwelcome to \"tooling up,\" a monthly colu...   \n1  0:00 am    by \\n\\n've always hated the term \"cover lett...   \n2  8:00 am    by \\n\\ne all know that, with respect to fash...   \n\n                                                 bio  date_seq  month_seq  \\\n0  [peter fiske is a ph.d. scientist and co-found...        18         10   \n1  [peter fiske is a ph.d. scientist and co-found...        81         12   \n2  [peter fiske is a ph.d. scientist and co-found...       361         21   \n\n   year       author  n_posts_author column1 column2 advice one_time  \n0  1996  peter fiske              59      no     yes    yes       no  \n1  1996  peter fiske              59      no     yes    yes       no  \n2  1997  peter fiske              59      no     yes    yes       no  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>headline</th>\n      <th>tags</th>\n      <th>authors</th>\n      <th>date</th>\n      <th>time</th>\n      <th>text</th>\n      <th>bio</th>\n      <th>date_seq</th>\n      <th>month_seq</th>\n      <th>year</th>\n      <th>author</th>\n      <th>n_posts_author</th>\n      <th>column1</th>\n      <th>column2</th>\n      <th>advice</th>\n      <th>one_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>5839</td>\n      <td>how to write a winning résumé</td>\n      <td>[tooling up, advice, graduate, academic, indus...</td>\n      <td>peter fiske</td>\n      <td>1996-10-18</td>\n      <td>8:00 am</td>\n      <td>by \\nwelcome to \"tooling up,\" a monthly colu...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>18</td>\n      <td>10</td>\n      <td>1996</td>\n      <td>peter fiske</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5835</td>\n      <td>the commandments of cover letter creation</td>\n      <td>[tooling up, advice, americas]</td>\n      <td>peter fiske</td>\n      <td>1996-12-20</td>\n      <td>0:00 am</td>\n      <td>by \\n\\n've always hated the term \"cover lett...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>81</td>\n      <td>12</td>\n      <td>1996</td>\n      <td>peter fiske</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5840</td>\n      <td>dressing scientists for success: male case study</td>\n      <td>[tooling up, advice, graduate, postdoc, academ...</td>\n      <td>peter fiske</td>\n      <td>1997-09-26</td>\n      <td>8:00 am</td>\n      <td>by \\n\\ne all know that, with respect to fash...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>361</td>\n      <td>21</td>\n      <td>1997</td>\n      <td>peter fiske</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# full_advice = \"../data/genre_advice_full_021520.jl\"\n",
    "full_filename = \"../data/by_article_fulltext_020920.jl\"\n",
    "\n",
    "# get full text dataset as a df\n",
    "# advice_df = imp.init_df(full_advice, \"full\", genre=\"advice\")\n",
    "# advice_df = advice_df[advice_df[\"year\"]<2020]\n",
    "full_df = imp.init_df(full_filename, \"full\")\n",
    "full_df = full_df[full_df[\"year\"]<2020]\n",
    "\n",
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#LOAD DOCUMENT BY LINE INTO AN ARRAY\n",
    "\n",
    "# Need to make separate docs for each unit of analysis\n",
    "\n",
    "\n",
    "# files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/txt/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "line_num = [i for i in range(len(full_df))]\n",
    "section = [\"advice\",\"news\",\"wl\"]\n",
    "\n",
    "####################\n",
    "### dictionaries ###\n",
    "####################\n",
    "\n",
    "all_words = []\n",
    "\n",
    "year_words = {}\n",
    "for i in range(1996,2020):\n",
    "    year_words[str(i)] = []\n",
    "    # print(i)\n",
    "\n",
    "sect_words = {}\n",
    "for j in section:\n",
    "    sect_words[j] = []\n",
    "\n",
    "sect_year_words = {}\n",
    "for i in range(1996,2020):\n",
    "    for j in section:\n",
    "        sect_year_words[str(j)+\"_\"+str(i)] = []\n",
    "\n",
    "for num in line_num:\n",
    "    # outpath = 'digitalSTS/papers/lines/' + filename + '.csv'\n",
    "    # w_test = full_df.iloc[num][\"text\"]\n",
    "    # with open(outpath, 'w') as outfile:\n",
    "        \n",
    "#         for line in infile.readlines():\n",
    "#             print line\n",
    "    paragraphs = full_df.iloc[num][\"text\"]\n",
    "    # print(paragraphs)\n",
    "    year = full_df.iloc[num][\"year\"]\n",
    "    if full_df.iloc[num][\"advice\"]==\"yes\":\n",
    "        sect=\"advice\"\n",
    "    elif full_df.iloc[num][\"one_time\"]==\"yes\":\n",
    "        sect=\"wl\"\n",
    "    else:\n",
    "        sect=\"news\"\n",
    "\n",
    "    for paragraph in paragraphs.split(\"\\n\"):\n",
    "        # print(\"=====\")\n",
    "        # print(paragraph)\n",
    "        if paragraph == \"\" or paragraph.strip() == \" \" or paragraph.strip() == \"by\":\n",
    "            continue\n",
    "\n",
    "        # paragraph = paragraph.decode('unicode_escape').encode('ascii','ignore')\n",
    "        lines = [line.strip().replace(\"--\",\" \") for line in re.split(\".?!\",paragraph)]#''.join(paragraph).split('.')]\n",
    "        #             print paragraph\n",
    "        \n",
    "        # writer = csv.writer(outfile)\n",
    "\n",
    "        for line in lines:\n",
    "            # print(\"#####\")\n",
    "            # print(line)\n",
    "            # it may be tempting to seek out a bargain discount men's clothing store, but you tend to get what you pay for\n",
    "            # line = line.replace(\"-\",\" \").replace(\"―\",\" \").replace(\"ē\",\"e\").replace(\" \",\" \").replace(\"ﬁ\",\"fi\").replace(\"β\",\"beta\").replace(\"►\",\"\").replace(\"‐\",\" \")\n",
    "            words = re.split(\" \",line)\n",
    "\n",
    "            if len(words) > 1:\n",
    "                # print(words)\n",
    "                all_words.append(line)\n",
    "                year_words[str(year)].append(line)\n",
    "                sect_words[sect].append(line)\n",
    "                sect_year_words[str(sect)+\"_\"+str(year)].append(line)\n",
    "                # full2df[\"id\"].append(full_df.iloc[num][\"id\"])\n",
    "                # full2df[\"line_words\"].append(words)\n",
    "                # full2df[\"type\"].append(ty)\n",
    "\n",
    "                #     writer.writerow([line.encode('utf-8')])\n",
    "# print(len(year_words[\"2010\"]))\n",
    "# print(len(sect_words[\"advice\"]))\n",
    "# print(sect_words[\"news\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "outfilename = \"../data/dictionary_files/all_text/txts/all_text.txt\"\n",
    "with open(outfilename, \"w\",encoding='utf-8') as outfile:\n",
    "    for line in all_words:\n",
    "        line += \"\\n\"\n",
    "        # line = line.encode('utf-8')\n",
    "        # print(line)\n",
    "        outfile.write(line)\n",
    "\n",
    "\n",
    "for key in sect_words.keys():\n",
    "    outfilename = \"../data/dictionary_files/section/txts/\"+key+\".txt\"\n",
    "    with open(outfilename, \"w\",encoding='utf-8') as outfile:\n",
    "        for line in sect_words[key]:\n",
    "            line += \"\\n\"\n",
    "            # line = line.encode('utf-8')\n",
    "            # print(line)\n",
    "            outfile.write(line)\n",
    "\n",
    "for key in year_words.keys():\n",
    "    outfilename = \"../data/dictionary_files/year/txts/\"+key+\".txt\"\n",
    "    with open(outfilename, \"w\",encoding='utf-8') as outfile:\n",
    "        for line in year_words[key]:\n",
    "            line += \"\\n\"\n",
    "            # line = line.encode('utf-8')\n",
    "            # print(line)\n",
    "            outfile.write(line)\n",
    "\n",
    "for key in sect_year_words.keys():\n",
    "    outfilename = \"../data/dictionary_files/section_year/txts/\"+key+\".txt\"\n",
    "    with open(outfilename, \"w\",encoding='utf-8') as outfile:\n",
    "        for line in sect_year_words[key]:\n",
    "            line += \"\\n\"\n",
    "            # line = line.encode('utf-8')\n",
    "            # print(line)\n",
    "            outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('../data/dictionary_files/sec/txts/') if not filename.startswith('.')]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionaries and Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../data/dictionary_files/all_text/txts/all_text.txt\nDictionary(25409 unique tokens: ['tooling', 'welcome', 'applicant', 'clearly', 'goals']...)\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "files = [filename.split('.')[0] for filename in os.listdir('../data/dictionary_files/all_text/txts/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = '../data/dictionary_files/all_text/txts/' + filename + '.txt'\n",
    "    print(inpath)\n",
    "\n",
    "    with open(inpath, 'r',encoding='utf-8') as infile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        documents = [line[0] for line in reader]\n",
    "#         print lines\n",
    "\n",
    "        # TOKENIZING\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "        \n",
    "        # GATHERING CANDIDATES FOR CUSTOM LIST OF STOPWORDS FROM NGRAMS\n",
    "        # ['i','m','a','a','i','t','6',...]\n",
    "        unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "        # ['do','or','or','of','be',...]\n",
    "        bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "        misc = ['get', 'know', 'would']\n",
    "        \n",
    "        # CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "        # REMOVES STOP WORDS\n",
    "        documents = [[token for token in doc if token not in stoplist]\n",
    "                        for doc in documents]\n",
    "\n",
    "        # REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "        documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                        for doc in documents ]\n",
    "        \n",
    "        # REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "        token_frequency = defaultdict(int)\n",
    "\n",
    "        # count all tokens\n",
    "        for doc in documents:\n",
    "            for token in doc:\n",
    "                token_frequency[token] += 1\n",
    "\n",
    "        # keep words that occur more than once\n",
    "        documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                        for doc in documents  ]\n",
    "\n",
    "        # Sort words in documents\n",
    "        for doc in documents:\n",
    "            doc.sort()\n",
    "            \n",
    "        # # HATE UNICODE!!\n",
    "        # # cuments = [ [token.dor token in doc] for doc in documents  ]\n",
    "  #        \n",
    "        # Build a dictionary where for each document each word has its own id\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        dictionary.compactify()\n",
    "        # and save the dictionary for future use\n",
    " \n",
    "        print(dictionary)\n",
    "        dictionary.save( '../data/dictionary_files/all_text/dicts/' + filename + '.dict')\n",
    "\n",
    "      \n",
    "        # BUILD AND SAVE CORPUS\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    " \n",
    "        corpora.MmCorpus.serialize('../data/dictionary_files/all_text/corpuses/' + filename + '.mm', corpus)\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE HELP FROM:\n",
    "# https://github.com/alexperrier/datatalks/blob/master/twitter/twitter_preprocessing.py\n",
    "\n",
    "# SPLITS EACH POST INTO WORDS WHILE REMOVING PUNCTUATION (\\w+) AND LOWER CASES THEM A WORD SUCH AS\n",
    "# I'M WILL BECOME I, M\n",
    "# test = [[doc for doc in docu] for docu in documents ]\n",
    "# print(\"hi\")\n",
    "# print(test[:3])\n",
    "documents_one = [\" \".join(doc) for doc in documents]\n",
    "documents = documents_one\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "## [['what', 'do', 'they', 'look', 'for', 'for', 'approval', 'or', 'denial'],\n",
    "##  ['hi','i','m','wondering','if','someone','could','answer',...]...\n",
    "# documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n[]\n"
    }
   ],
   "source": [
    "# I'M NOT SURE THESE ARE TECHNICALLY UNIGRAMS AND BIGRAMS SO MUCH AS THEY ARE SINGLE LETTER WORDS\n",
    "# AND TWO LETTER WORDS BUT WHO AM I TO JUDEGE (SHULDER SHURG)\n",
    "\n",
    "# ['i','m','a','a','i','t','6',...]\n",
    "unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "# ['do','or','or','of','be',...]\n",
    "bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "misc = ['get', 'know', 'would']\n",
    "\n",
    "print(unigrams[:5])\n",
    "print(bigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['tooling', 'welcome']\n['applicant', 'clearly', 'goals', 'states']\n"
    }
   ],
   "source": [
    "# CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "# REMOVES STOP WORDS\n",
    "documents = [[token for token in doc if token not in stoplist]\n",
    "                for doc in documents]\n",
    "\n",
    "# REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                for doc in documents ]\n",
    "\n",
    "# # [['look', 'approval', 'denial', 'parole', 'approval'...],\n",
    "# # ['wondering', 'someone', 'could', 'answer', 'question'...]...]\n",
    "print(documents[0][:5])\n",
    "print(documents[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['tooling', 'welcome']\n['applicant', 'clearly', 'goals', 'states']\n"
    }
   ],
   "source": [
    "# REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "token_frequency = defaultdict(int)\n",
    "\n",
    "# count all tokens\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# keep words that occur more than once\n",
    "documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                for doc in documents  ]\n",
    "\n",
    "# Sort words in documents\n",
    "for doc in documents:\n",
    "    doc.sort()\n",
    "    \n",
    "## [['approval', 'approval', 'approval', 'approval', 'approval'...],\n",
    "##  ['answer', 'benefit', 'benefit', 'benefit', 'confused'...]...]\n",
    "\n",
    "print(documents[0][:5])\n",
    "print(documents[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academics', 'career', 'cheese', 'colleagues', 'early']\n",
      "['dollars', 'enter', 'health', 'make', 'public']\n"
     ]
    }
   ],
   "source": [
    "# I HATE UNICODE!!\n",
    "# documents = [ [token.decode('ascii', 'ignore') for token in doc] for doc in documents  ]\n",
    "print(documents[0][:5])\n",
    "print(documents[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary where for each document each word has its own id\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "# and save the dictionary for future use\n",
    "dictionary.save('all_text.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dictionary(25409 unique tokens: ['tooling', 'welcome', 'applicant', 'clearly', 'goals']...)\n"
    }
   ],
   "source": [
    "# We now have a dictionary with 11127 unique tokens\n",
    "# Dictionary(11127 unique tokens: [u'foul', u'narcotic', u'four', u'woods', u'hanging']...)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the corpus: vectors with occurence of each word for each document\n",
    "# convert tokenized documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save in Market Matrix format\n",
    "corpora.MmCorpus.serialize('all_text.mm', corpus)\n",
    "# this corpus can be loaded with corpus = corpora.MmCorpus('prison_talk_nyc.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../data/dictionary_files/years/txts/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-90765fa86dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/dictionary_files/years/txts/'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../data/dictionary_files/years/txts/'"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('../data/dictionary_files/years/txts/') if not filename.startswith('.')]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/dictionary_files/years/txts/advice.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-4c384f127ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moutpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.../data/dictionary_files/years/lemmas/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0messay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/dictionary_files/years/txts/advice.txt'"
     ]
    }
   ],
   "source": [
    "# filename = '02_Parmiggiani_Monteiro'\n",
    "# testpath = '../papers/tests/' + '02_Parmiggiani_Monteiro' + '.csv'\n",
    "\n",
    "totalfq = collections.Counter()\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = '../data/dictionary_files/years/txts/' + filename + '.txt'\n",
    "    outpath = '.../data/dictionary_files/years/lemmas/' + filename + '.txt'\n",
    "    \n",
    "    with open(inpath, 'r') as infile, open(outpath, 'w') as outfile:\n",
    "\n",
    "        essay = infile.readline()\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(essay.lower())\n",
    "\n",
    "        # ['i','m','a','a','i','t','6',...]\n",
    "        unichars = [word for word in tokens if len(word)==1]\n",
    "        # ['do','or','or','of','be',...]\n",
    "        bichars  =  [word for word in tokens if len(word)==2]\n",
    "\n",
    "        misc = ['get', 'know', 'would', 'and', 'yet', 'due', 'one', 'digital', 'technology', 'way', \\\n",
    "                'also', 'use', 'within', 'like', 'sts', 'may', 'however', 'even', u'technology', 'used']\n",
    "\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unichars + bichars + misc)\n",
    "\n",
    "        # REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "        post_stop = [token for token in tokens if len(token.strip(digits)) == len(token)]\n",
    "\n",
    "        # REMOVES STOP WORDS\n",
    "        post_stop = [token for token in tokens if token not in stoplist]\n",
    "\n",
    "        post_lemma = [lemma.lemmatize(token) for token in post_stop]\n",
    "\n",
    "        tokenfq = collections.Counter()\n",
    "        tokenfq.update(post_lemma)\n",
    "        \n",
    "        post_highfq = [token for token in post_lemma if tokenfq[token] > 1]\n",
    "\n",
    "        totalfq.update(post_highfq)\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([' '.join(post_highfq).encode('utf-8')])\n",
    "\n",
    "        print(filename), '----> TOKENS IN:', len(tokens), 'POST STOP:', len(post_stop), 'POST HIGHFQ:', len(post_highfq), \\\n",
    "                        '----> PCT:', round(float(len(post_highfq))/float(len(tokens)), 2) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf6638f140b364320bf3d7f1da45245d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}