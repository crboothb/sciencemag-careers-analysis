{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create features from the raw text so we can train the machine learning models. The steps followed are:\n",
    "\n",
    "1. **Text Cleaning and Preparation**: cleaning of special characters, downcasing, punctuation signs. possessive pronouns and stop words removal and lemmatization. \n",
    "2. **Label coding**: creation of a dictionary to map each category to a code.\n",
    "3. **Train-test split**: to test the models on unseen data.\n",
    "4. **Text representation**: use of TF-IDF scores to represent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_func as imp\n",
    "import classifier_help as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we'll load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \"pickles/hand_coded_ALL.pickle\"\n",
    "full_filename = \"../data/by_article_fulltext_112919-2.jl\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    coded_df = pickle.load(data)\n",
    "\n",
    "df = imp.init_df(full_filename, \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>headline</th>\n      <th>tags</th>\n      <th>authors</th>\n      <th>date</th>\n      <th>time</th>\n      <th>text</th>\n      <th>bio</th>\n      <th>date_seq</th>\n      <th>month_seq</th>\n      <th>year</th>\n      <th>n_posts_author</th>\n      <th>column1</th>\n      <th>column2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>5865</td>\n      <td>how to write a winning résumé</td>\n      <td>[tooling up, advice, graduate, academic, indus...</td>\n      <td>peter fiske</td>\n      <td>1996-10-18</td>\n      <td>8:00 am</td>\n      <td>by  welcome to \"tooling up,\" a monthly colum...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>18</td>\n      <td>10</td>\n      <td>1996</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5872</td>\n      <td>the commandments of cover letter creation</td>\n      <td>[tooling up, advice, americas]</td>\n      <td>peter fiske</td>\n      <td>1996-12-20</td>\n      <td>0:00 am</td>\n      <td>by   've always hated the term \"cover letter...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>81</td>\n      <td>12</td>\n      <td>1996</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5874</td>\n      <td>dressing scientists for success: male case study</td>\n      <td>[tooling up, advice, graduate, postdoc, academ...</td>\n      <td>peter fiske</td>\n      <td>1997-09-26</td>\n      <td>8:00 am</td>\n      <td>by   e all know that, with respect to fashio...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>361</td>\n      <td>21</td>\n      <td>1997</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5876</td>\n      <td>the tooling up book club: on the market</td>\n      <td>[tooling up, advice, graduate, postdoc, academ...</td>\n      <td>peter fiske</td>\n      <td>1998-01-30</td>\n      <td>0:00 am</td>\n      <td>by   ow that the holiday season is over, you...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>487</td>\n      <td>25</td>\n      <td>1998</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5878</td>\n      <td>self-assessment exercises: a gre for your ego ...</td>\n      <td>[tooling up, advice, early career, graduate, m...</td>\n      <td>peter fiske</td>\n      <td>1998-02-27</td>\n      <td>0:00 am</td>\n      <td>by   --miguel de cervantes any young scienti...</td>\n      <td>[peter fiske is a ph.d. scientist and co-found...</td>\n      <td>515</td>\n      <td>26</td>\n      <td>1998</td>\n      <td>59</td>\n      <td>no</td>\n      <td>yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     id                                           headline  \\\n0  5865                      how to write a winning résumé   \n1  5872          the commandments of cover letter creation   \n2  5874   dressing scientists for success: male case study   \n3  5876            the tooling up book club: on the market   \n4  5878  self-assessment exercises: a gre for your ego ...   \n\n                                                tags      authors       date  \\\n0  [tooling up, advice, graduate, academic, indus...  peter fiske 1996-10-18   \n1                     [tooling up, advice, americas]  peter fiske 1996-12-20   \n2  [tooling up, advice, graduate, postdoc, academ...  peter fiske 1997-09-26   \n3  [tooling up, advice, graduate, postdoc, academ...  peter fiske 1998-01-30   \n4  [tooling up, advice, early career, graduate, m...  peter fiske 1998-02-27   \n\n      time                                               text  \\\n0  8:00 am    by  welcome to \"tooling up,\" a monthly colum...   \n1  0:00 am    by   've always hated the term \"cover letter...   \n2  8:00 am    by   e all know that, with respect to fashio...   \n3  0:00 am    by   ow that the holiday season is over, you...   \n4  0:00 am    by   --miguel de cervantes any young scienti...   \n\n                                                 bio  date_seq  month_seq  \\\n0  [peter fiske is a ph.d. scientist and co-found...        18         10   \n1  [peter fiske is a ph.d. scientist and co-found...        81         12   \n2  [peter fiske is a ph.d. scientist and co-found...       361         21   \n3  [peter fiske is a ph.d. scientist and co-found...       487         25   \n4  [peter fiske is a ph.d. scientist and co-found...       515         26   \n\n   year  n_posts_author column1 column2  \n0  1996              59      no     yes  \n1  1996              59      no     yes  \n2  1997              59      no     yes  \n3  1998              59      no     yes  \n4  1998              59      no     yes  "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize one sample news content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'  by   \\'ve always hated the term \"cover letter.\" it implies that the letter you send out to accompany your résumé, the opening shot in your job-hunt campaign, is merely \"decoration\" for your résumé. a good cover letter does far more than just cover: it engages the reader and makes her want to explore your job qualifications more fully. a good cover letter also highlights your qualifications, guides the reader through the most important parts of your work history, and demonstrates your flawless command of the english language. that\\'s a lot to cover in only three paragraphs! don\\'t be daunted. writing a good cover letter boils down to making a decent presentation of your experience with the employer\\'s needs foremost in mind. this point is extremely important. if you do not have a good understanding of the requirements of the position and the nature of the work involved, how will you be able to answer the needs of the employer? writing a good cover letter, just like preparing a winning résumé, requires research into the organization you are approaching. i have distilled eons of wisdom about cover letters into a list of 10 commandments (well, commandments is a little strong; how about recommendations?) that you should bear in mind while preparing your letters. even if you have already written your cover letter, you may want to use this list as a check on your product. this may seem obvious, but you must be specific from the outset about why you are sending someone your résumé. are you applying for an advertised position or just a potential opening? if it is a specific opening, where did you learn about it? you would be surprised how many people fail to mention the specific job to which they are applying. for big companies that are advertising many positions, your lack of specificity may land your résumé in the recycle bin. even for small operations, it is important to explain how you heard about the job. most mediocre cover letters are not specific. they cite items in the résumé but they fail to make the connection to the job that is being advertised. sure, it\\'s a great thing that you have worked for a summer doing data reduction, but what if they are most concerned about project management experience? you\\'re hosed! you have to show them how your background and experience fit the job they are advertising. at the bare minimum, you should have the job advertisement in front of you as you are writing. but you will be further ahead if you\\'ve actually done some research on the company, or better yet, have talked to the people who are advertising the opening. the best way to prove that you fit is to cite examples in your past work history where you tackled similar job duties or occupied a similar position. do   assume that they will pick these details out of your résumé, especially as the average employer spends only 20 seconds scanning through a résumé. for example, if you are applying to work in an aeronautical engineering company doing product development, you want to note any specific experience involving the development of a device or experiment rather than just citing your years of experimental work for your ph.d. citing specific examples and quantifying them where possible is the best way of convincing a stranger that you\\'ve got what it takes to get the job done. read the job description and other materials carefully. one way to make your background and experience a better fit is to use the same phrases and descriptors that they use in the job advertisement. this is an important aspect of \"speaking the same language\" as the employer. if you use the same terms, you will make a more effective connection between your experience and their needs. directing cover letters to nameless human resources personnel is like asking the crocodiles in the moat to lower the drawbridge! hr people are best at scanning résumés and matching job descriptors with items on people\\'s résumés. they are not very good at figuring out how someone with an unusual background (like having a ph.d.) will fit into a particular position. that decision is left with the hiring manager. in some cases, especially with large companies, it is impossible to do a complete end run around the hr department. in those cases, the best you can do is send a duplicate cover letter and résumé to the person who is actually making the hiring decision. don\\'t know who that is? call and find out! it doesn\\'t take clairvoyance to guess what questions might be uppermost in the mind of someone who reads your cover letter. if you are applying for a job for which a ph.d. is not required, one obvious question is: \"why is a ph.d. applying for this job?\" another question might be: \"is this person overqualified?\" it is important to anticipate these questions and allay any concerns in your cover letter. it is important to show how your experience as a scientist would be an asset in a wide variety of positions. one-page cover letters are a rule unless you have some specific reasons to make them longer (for example, if the job description or advertisement asks for answers to questions or for more information). in fact, three paragraphs should be sufficient. if your letter is any longer, you\\'d better have a good reason. here is a suggested general structure: a sad story about how you have been searching for a job for the last 4 years may be sincere and truthful, but it rarely makes a good impression. a good cover letter should project an image of confidence and professionalism. save the confessions until after you get the job. your cover letter should be an example of your best written communication. therefore, it should be self-evident that the writing should be clear, distinctive, and devoid of clichés. but many people can\\'t help but insert some stock cover letter phrases such as: \"enclosed please find ...\" or \"thank you for your consideration.\" these phrases are so routine in cover letters! say those things differently! be unique! show some flair! some companies are using electronic résumé tracking for all their applicants. many times they don\\'t bother scanning in the cover letter at all and simply throw it away. spending a great deal of time on a cover letter in these cases is a waste. it would be far wiser to find out the name of the person who is the hiring manager and contact them separately in a letter, either with or without a copy of your résumé. the personal contact and attention to detail that a separate letter represents is important. there is a final transcendent rule for sending out your job materials:  ! you would be surprised how many people simply mail off their materials and never bother to check back with the employer to see if they have actually arrived. following up with a phone call a week or so after mailing your job materials does two things. first, it ensures that your letter arrived. more importantly, it demonstrates that you are able to follow through and shows that you are genuinely interested in the job. i have heard from several hiring managers who have been impressed by such a simple act.  and co-author, with dr. geoff davis, of a blog (at phds.org) on science policy, economics, and educational initiatives that affect science employment. fiske lives with his wife and two daughters in oakland, california, and is a frequent lecturer on the subject of career development for scientists.'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since this notebook uses the column heading content and I don't feel like changing every single one yet\n",
    "df[\"Content\"] = df[\"text\"]\n",
    "\n",
    "df.loc[1]['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Special character cleaning\n",
    "\n",
    "We can see the following special characters:\n",
    "\n",
    "* ``\\r``\n",
    "* ``\\n``\n",
    "* ``\\`` before possessive pronouns (`government's = government\\'s`)\n",
    "* ``\\`` before possessive pronouns 2 (`Yukos'` = `Yukos\\'`)\n",
    "* ``\"`` when quoting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "14\n"
    }
   ],
   "source": [
    "# \\r and \\n\n",
    "df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\'s'\", \"\")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\'\", \"\")\n",
    "# because I still need quotation marks\n",
    "print(df.loc[1][\"Content_Parsed_1\"].count(\"\\\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Upcase/downcase\n",
    "\n",
    "I already did this when importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Punctuation signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation signs won't have any predicting power, so we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation_signs = list(\"?:!.,;\")\n",
    "# df['Content_Parsed_2'] = df['Content_Parsed_1']\n",
    "\n",
    "# for punct_sign in punctuation_signs:\n",
    "#     df['Content_Parsed_2'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "\n",
    "## Instead use my own function\n",
    "\n",
    "df[\"Content_Parsed_3\"] = cls.no_punctuation(df[\"Content_Parsed_1\"], quotes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this we are messing up with some numbers, but it's no problem since we aren't expecting any predicting power from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Possessive pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove possessive pronoun terminations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Stemming and Lemmatization\n",
    "\n",
    "Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, so it will be more useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "------------------------------------------------------------\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Clara\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Clara\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to lemmatize, we have to iterate through every word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although lemmatization doesn't work perfectly in all cases (as can be seen in the example below), it can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Clara\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the stop words, we'll handle a regular expression only detecting whole words, as seen in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'StopWord eating a meal'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"me eating a meal\"\n",
    "word = \"me\"\n",
    "\n",
    "# The regular expression is:\n",
    "regex = r\"\\b\" + word + r\"\\b\"  # we need to build it like that to work properly\n",
    "\n",
    "re.sub(regex, \"StopWord\", example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop through all the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some dobule/triple spaces between words because of the replacements. However, it's not a problem because we'll tokenize by the spaces later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we'll show an original news article and its modifications throughout the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Japan narrowly escapes recession\\r\\n\\r\\nJapan\\'s economy teetered on the brink of a technical recession in the three months to September, figures show.\\r\\n\\r\\nRevised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\\r\\n\\r\\nThe government was keen to play down the worrying implications of the data. \"I maintain the view that Japan\\'s economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It\\'s painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Special character cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Japan narrowly escapes recession Japans economy teetered on the brink of a technical recession in the three months to September, figures show. Revised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth. The government was keen to play down the worrying implications of the data. I maintain the view that Japans economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully, said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. Its painting a picture of a recovery... much patchier than previously thought, said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Upcase/downcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'japan narrowly escapes recession japans economy teetered on the brink of a technical recession in the three months to september, figures show. revised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. on an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. a common technical definition of a recession is two successive quarters of negative growth. the government was keen to play down the worrying implications of the data. i maintain the view that japans economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully, said economy minister heizo takenaka. but in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. its painting a picture of a recovery... much patchier than previously thought, said paul sheard, economist at lehman brothers in tokyo. improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'japan narrowly escapes recession japans economy teetered on the brink of a technical recession in the three months to september figures show revised figures indicated growth of just 01% - and a similar-sized contraction in the previous quarter on an annual basis the data suggests annual growth of just 02% suggesting a much more hesitant recovery than had previously been thought a common technical definition of a recession is two successive quarters of negative growth the government was keen to play down the worrying implications of the data i maintain the view that japans economy remains in a minor adjustment phase in an upward climb and we will monitor developments carefully said economy minister heizo takenaka but in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead observers were less sanguine its painting a picture of a recovery much patchier than previously thought said paul sheard economist at lehman brothers in tokyo improvements in the job market apparently have yet to feed through to domestic demand with private consumption up just 02% in the third quarter'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Possessive pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'japan narrowly escapes recession japans economy teetered on the brink of a technical recession in the three months to september figures show revised figures indicated growth of just 01% - and a similar-sized contraction in the previous quarter on an annual basis the data suggests annual growth of just 02% suggesting a much more hesitant recovery than had previously been thought a common technical definition of a recession is two successive quarters of negative growth the government was keen to play down the worrying implications of the data i maintain the view that japans economy remains in a minor adjustment phase in an upward climb and we will monitor developments carefully said economy minister heizo takenaka but in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead observers were less sanguine its painting a picture of a recovery much patchier than previously thought said paul sheard economist at lehman brothers in tokyo improvements in the job market apparently have yet to feed through to domestic demand with private consumption up just 02% in the third quarter'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'japan narrowly escape recession japan economy teeter on the brink of a technical recession in the three months to september figure show revise figure indicate growth of just 01% - and a similar-sized contraction in the previous quarter on an annual basis the data suggest annual growth of just 02% suggest a much more hesitant recovery than have previously be think a common technical definition of a recession be two successive quarter of negative growth the government be keen to play down the worry implications of the data i maintain the view that japan economy remain in a minor adjustment phase in an upward climb and we will monitor developments carefully say economy minister heizo takenaka but in the face of the strengthen yen make export less competitive and indications of weaken economic condition ahead observers be less sanguine its paint a picture of a recovery much patchier than previously think say paul sheard economist at lehman brothers in tokyo improvements in the job market apparently have yet to fee through to domestic demand with private consumption up just 02% in the third quarter'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'japan narrowly escape recession japan economy teeter   brink   technical recession   three months  september figure show revise figure indicate growth   01% -   similar-sized contraction   previous quarter   annual basis  data suggest annual growth   02% suggest  much  hesitant recovery   previously  think  common technical definition   recession  two successive quarter  negative growth  government  keen  play   worry implications   data  maintain  view  japan economy remain   minor adjustment phase   upward climb    monitor developments carefully say economy minister heizo takenaka    face   strengthen yen make export less competitive  indications  weaken economic condition ahead observers  less sanguine  paint  picture   recovery much patchier  previously think say paul sheard economist  lehman brothers  tokyo improvements   job market apparently  yet  fee   domestic demand  private consumption   02%   third quarter'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can delete the intermediate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File_Name</th>\n      <th>Content</th>\n      <th>Category</th>\n      <th>Complete_Filename</th>\n      <th>id</th>\n      <th>News_length</th>\n      <th>Content_Parsed_1</th>\n      <th>Content_Parsed_2</th>\n      <th>Content_Parsed_3</th>\n      <th>Content_Parsed_4</th>\n      <th>Content_Parsed_5</th>\n      <th>Content_Parsed_6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>001.txt</td>\n      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n      <td>business</td>\n      <td>001.txt-business</td>\n      <td>1</td>\n      <td>2569</td>\n      <td>Ad sales boost Time Warner profit Quarterly pr...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  File_Name                                            Content  Category  \\\n0   001.txt  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...  business   \n\n  Complete_Filename  id  News_length  \\\n0  001.txt-business   1         2569   \n\n                                    Content_Parsed_1  \\\n0  Ad sales boost Time Warner profit Quarterly pr...   \n\n                                    Content_Parsed_2  \\\n0  ad sales boost time warner profit quarterly pr...   \n\n                                    Content_Parsed_3  \\\n0  ad sales boost time warner profit quarterly pr...   \n\n                                    Content_Parsed_4  \\\n0  ad sales boost time warner profit quarterly pr...   \n\n                                    Content_Parsed_5  \\\n0  ad sales boost time warner profit quarterly pr...   \n\n                                    Content_Parsed_6  \n0  ad sales boost time warner profit quarterly pr...  "
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
    "df = df[list_columns]\n",
    "\n",
    "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File_Name</th>\n      <th>Category</th>\n      <th>Complete_Filename</th>\n      <th>Content</th>\n      <th>Content_Parsed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>001.txt</td>\n      <td>business</td>\n      <td>001.txt-business</td>\n      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>002.txt</td>\n      <td>business</td>\n      <td>002.txt-business</td>\n      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n      <td>dollar gain  greenspan speech  dollar  hit  hi...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>003.txt</td>\n      <td>business</td>\n      <td>003.txt-business</td>\n      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n      <td>yukos unit buyer face loan claim  owners  emba...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>004.txt</td>\n      <td>business</td>\n      <td>004.txt-business</td>\n      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n      <td>high fuel price hit bas profit british airways...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>005.txt</td>\n      <td>business</td>\n      <td>005.txt-business</td>\n      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n      <td>pernod takeover talk lift domecq share  uk dri...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  File_Name  Category Complete_Filename  \\\n0   001.txt  business  001.txt-business   \n1   002.txt  business  002.txt-business   \n2   003.txt  business  003.txt-business   \n3   004.txt  business  004.txt-business   \n4   005.txt  business  005.txt-business   \n\n                                             Content  \\\n0  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...   \n1  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...   \n2  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...   \n3  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...   \n4  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...   \n\n                                      Content_Parsed  \n0  ad sales boost time warner profit quarterly pr...  \n1  dollar gain  greenspan speech  dollar  hit  hi...  \n2  yukos unit buyer face loan claim  owners  emba...  \n3  high fuel price hit bas profit british airways...  \n4  pernod takeover talk lift domecq share  uk dri...  "
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "\n",
    "We need to remember that our model will gather the latest news articles from different newspapers every time we want. For that reason, we not only need to take into account the peculiarities of the training set articles, but also possible ones that are present in the gathered news articles.\n",
    "\n",
    "For this reason, possible peculiarities have been studied in the *05. News Scraping* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dictionary with the label codification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping\n",
    "df['Category_Code'] = df['Category']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File_Name</th>\n      <th>Category</th>\n      <th>Complete_Filename</th>\n      <th>Content</th>\n      <th>Content_Parsed</th>\n      <th>Category_Code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>001.txt</td>\n      <td>business</td>\n      <td>001.txt-business</td>\n      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n      <td>ad sales boost time warner profit quarterly pr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>002.txt</td>\n      <td>business</td>\n      <td>002.txt-business</td>\n      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n      <td>dollar gain  greenspan speech  dollar  hit  hi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>003.txt</td>\n      <td>business</td>\n      <td>003.txt-business</td>\n      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n      <td>yukos unit buyer face loan claim  owners  emba...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>004.txt</td>\n      <td>business</td>\n      <td>004.txt-business</td>\n      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n      <td>high fuel price hit bas profit british airways...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>005.txt</td>\n      <td>business</td>\n      <td>005.txt-business</td>\n      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n      <td>pernod takeover talk lift domecq share  uk dri...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  File_Name  Category Complete_Filename  \\\n0   001.txt  business  001.txt-business   \n1   002.txt  business  002.txt-business   \n2   003.txt  business  003.txt-business   \n3   004.txt  business  004.txt-business   \n4   005.txt  business  005.txt-business   \n\n                                             Content  \\\n0  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...   \n1  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...   \n2  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...   \n3  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...   \n4  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...   \n\n                                      Content_Parsed  Category_Code  \n0  ad sales boost time warner profit quarterly pr...              0  \n1  dollar gain  greenspan speech  dollar  hit  hi...              0  \n2  yukos unit buyer face loan claim  owners  emba...              0  \n3  high fuel price hit bas profit british airways...              0  \n4  pernod takeover talk lift domecq share  uk dri...              0  "
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set apart a test set to prove the quality of our models. We'll do Cross Validation in the train set in order to tune the hyperparameters and then test performance on the unseen data of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1891 1891 2225\n"
    }
   ],
   "source": [
    "print(len(X_train), len(y_train), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have much observations (only 2.225), we'll choose a test set size of 15% of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have various options:\n",
    "\n",
    "* Count Vectors as features\n",
    "* TF-IDF Vectors as features\n",
    "* Word Embeddings as features\n",
    "* Text / NLP based features\n",
    "* Topic Models as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **TF-IDF Vectors** as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the different parameters:\n",
    "\n",
    "* `ngram_range`: We want to consider both unigrams and bigrams.\n",
    "* `max_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold\n",
    "* `min_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold.\n",
    "* `max_features`: If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "See `TfidfVectorizer?` for further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It needs to be mentioned that we are implicitly scaling our data when representing it as TF-IDF features with the argument `norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen these values as a first approximation. Since the models that we develop later have a very good predictive power, we'll stick to these values. But it has to be mentioned that different combinations could be tried in order to improve even more the accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(1891, 300)\n(334, 300)\n"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5\n"
    },
    {
     "data": {
      "text/plain": "dict_items([('business', 0), ('entertainment', 1), ('politics', 2), ('sport', 3), ('tech', 4)])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(category_codes))\n",
    "\n",
    "category_codes.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we have fitted and then transformed the training set, but we have **only transformed** the **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Chi squared test in order to see what unigrams and bigrams are most correlated with each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "# 'business' category:\n  . Most correlated unigrams:\n. market\n. economy\n. growth\n. oil\n. bank\n  . Most correlated bigrams:\n. last year\n. year old\n\n# 'entertainment' category:\n  . Most correlated unigrams:\n. best\n. music\n. star\n. award\n. film\n  . Most correlated bigrams:\n. tell bbc\n. prime minister\n\n# 'politics' category:\n  . Most correlated unigrams:\n. minister\n. blair\n. election\n. party\n. labour\n  . Most correlated bigrams:\n. tell bbc\n. prime minister\n\n# 'sport' category:\n  . Most correlated unigrams:\n. win\n. side\n. game\n. team\n. match\n  . Most correlated bigrams:\n. say mr\n. year old\n\n# 'tech' category:\n  . Most correlated unigrams:\n. digital\n. technology\n. computer\n. software\n. users\n  . Most correlated bigrams:\n. year old\n. say mr\n\n"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the unigrams correspond well to their category. However, bigrams do not. If we get the bigrams in our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['tell bbc', 'last year', 'prime minister', 'year old', 'say mr']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are only six. This means the unigrams have more correlation with the category than the bigrams, and since we're restricting the number of features to the most representative 300, only a few bigrams are being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the files we'll need in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}